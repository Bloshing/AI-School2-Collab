{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHEGHmkuLrLa"
      },
      "source": [
        "<h1>AI School 2</h1>\n",
        "\n",
        "<h2>Overview</h2>\n",
        "\n",
        "- Feature Engineering\n",
        "    - What is Feature Engineering?\n",
        "    - Training and Testing Logic\n",
        "    - Different Types of Models\n",
        "- Neural Networks\n",
        "    - What is a Neural Network?\n",
        "    - What is Gradient Descent?\n",
        "    - What are Nets?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mk_JEd2CLrLa"
      },
      "source": [
        "# AI School 2 Pre-Test Survey\n",
        "\n",
        "Form: https://forms.gle/sq9R3xTHe4hHvLvB6\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ix7Hl6WYLrLb"
      },
      "source": [
        "#### Pip Installs if necessary!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJIho6HeLrLb"
      },
      "outputs": [],
      "source": [
        "!pip install scikit-learn\n",
        "!pip install torch\n",
        "!pip install torchvision"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eN4cZ0J5MYMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "W60_U4HhLrLb"
      },
      "outputs": [],
      "source": [
        "#%pip install scikit-learn\n",
        "#%pip install torch\n",
        "#%pip install torchvision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBVnP3cMLrLc"
      },
      "source": [
        "<h1>What is Feature Engineering?</h1>\n",
        "\n",
        "Feature engineering is the process of using data to create, transform, and select features to enhance a model.\n",
        "\n",
        "You can think of Features as Columns on a Data Set!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ok8JXHpzLrLc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88uffLl9LrLc"
      },
      "source": [
        "<h1>Imports and Libraries</h1>\n",
        "\n",
        "In Python, there are these things called Imports and Modules. In Python, you can import a variety of files, packages, modules, and libraries!\n",
        "\n",
        "For Feature Engineering, there are a variety of libraries we can use but the most common are:\n",
        "\n",
        "* NumPy\n",
        "* Pandas\n",
        "* Scikit-Learn\n",
        "* PyTorch\n",
        "\n",
        "But there are even more for whatever task you want to do using Python Code!\n",
        "\n",
        "\n",
        "You can think of Features as Columns on a Data Set!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdEY_jMLLrLc"
      },
      "source": [
        "In order to import these libaries it should look like this:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "```\n",
        "\n",
        "In the following blank code cell, import the following modules:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpsVaHC3LrLc"
      },
      "source": [
        "Now there are some modules that are so big that they might crash your laptop.\n",
        "\n",
        "Some of these are Scikit-Learn and PyTorch.\n",
        "\n",
        "Due to this, we will import individual packages from them rather than all of scikit learn for learning purposes and to save time.\n",
        "\n",
        "Normally if you publish code onto GitHub or any other Cloud platform, it is standard to use it all under one cell.\n",
        "\n",
        "For teaching demonstrations and learning purposes, if we use a specific module, you will also see its import statement as well.\n",
        "\n",
        "But for now, let's get back to Feature Engineering!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPiQnw1WLrLd"
      },
      "source": [
        "<h1> Feature Engineering</h1>\n",
        "\n",
        "For feature engineering, we can think of features as columns.\n",
        "\n",
        "Run the following cell to see the dataset!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNwn1EhkLrLd"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvIrpf_aLrLd"
      },
      "source": [
        "We will be using the MNIST Dataset which is a Dataset with a bunch of hand-drawn number drawings that have turned into code.\n",
        "\n",
        "The MNIST Dataset has 70,000 Images and for learning purposes and time effiency we will be working with 1000 images!\n",
        "\n",
        "We will use different models for Image Classification.\n",
        "\n",
        "Specifically, which model is the best model for predicting numbers on the images!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajCF_a5_LrLd"
      },
      "outputs": [],
      "source": [
        "mnist = fetch_openml('mnist_784', as_frame=False)\n",
        "X = mnist.data[:1000]\n",
        "y= mnist.target.astype(int)[:1000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4MPfJz2LrLd"
      },
      "outputs": [],
      "source": [
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jz71ee2nLrLd"
      },
      "outputs": [],
      "source": [
        "y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xnx1swBRLrLd"
      },
      "source": [
        "## Training and Testing Logic\n",
        "\n",
        "For Training and Testing a Model, you must ask yourself how many times do I want to test my model?\n",
        "\n",
        "Due to the limiting computer powers of laptops, your answer might be once or twice.\n",
        "\n",
        "If its once, we will have a training and testing set that is split 70-30.\n",
        "\n",
        "About 70% of the data is spent on training and about 30% of the data is spent on testing.\n",
        "\n",
        "If its twice, we will have a training, validation, and holdout. The split is 70-15-15\n",
        "\n",
        "What is a Validation Set?\n",
        "\n",
        "A Validation Set is a Test Set that you have inbetween your Training Set and Final Test (Holdout) Set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Xe3cPsNLrLd"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9kABeePLrLd"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=41)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U94md8dyLrLd"
      },
      "outputs": [],
      "source": [
        "X_validation, X_holdout_test, y_validation, y_holdout_test = train_test_split(X_test, y_test, test_size=0.5, random_state=41)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTu4wYRWLrLe"
      },
      "source": [
        "Really Important to Scale Your Data\n",
        "\n",
        "It just helps normalize the data, change any labels, and helps increase model/performance speed.\n",
        "\n",
        "Time is our biggest enemy for Neural Networks and Feature Engineering so saving time is always a plus!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cz57xpq2LrLe"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zacAjvYXLrLe"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "scaled_X_train = scaler.fit_transform(X_train)\n",
        "scaled_X_test = scaler.transform(X_test)\n",
        "scaled_X_holdout_test = scaler.transform(X_holdout_test)\n",
        "scaled_X_validation = scaler.fit_transform(X_validation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnkiNWIdLrLe"
      },
      "source": [
        "Different Types of Models We Can Use Are\n",
        "\n",
        "* Logistic Regression\n",
        "* Support Vector Machine\n",
        "* Random Forest Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CB_c7f1YLrLe"
      },
      "source": [
        "Logistic Regression\n",
        "\n",
        "Each model has a lot of parameters you can change, but for Logistic Regression.\n",
        "\n",
        "The most important ones for Logistic Regression are\n",
        "\n",
        "* max_iter\n",
        "* solver\n",
        "* penalty\n",
        "* C : Inverse of Regularization Strength (The smaller the value is, the better it is)\n",
        "\n",
        "\n",
        "`max_iter` takes values from 1 to 1000\n",
        "\n",
        "`solver` takes values ['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga']\n",
        "\n",
        "`penalty` takes values ['l1', 'l2', 'elasticnet', None]\n",
        "\n",
        "`max_iter` takes positive values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlJdaGcHLrLe"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSJW6j_ULrLe"
      },
      "outputs": [],
      "source": [
        "log_model = LogisticRegression(solver='saga',max_iter=1000,random_state=41)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "R8OlPJCNLrLe"
      },
      "outputs": [],
      "source": [
        "log_model.fit(scaled_X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYUH1V4qLrLe"
      },
      "outputs": [],
      "source": [
        "log_validation_predictions = log_model.predict(scaled_X_validation)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqjJLTPpLrLe"
      },
      "outputs": [],
      "source": [
        "log_holdout_predictions = log_model.predict(scaled_X_holdout_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSQD0pwXLrLe"
      },
      "outputs": [],
      "source": [
        "log_y_final_pred = log_model.predict(scaled_X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AvGYV3LFLrLe"
      },
      "outputs": [],
      "source": [
        "accuracy_score(y_test,log_y_final_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlsCBGcBLrLe"
      },
      "source": [
        "Parameters you can change!\n",
        "\n",
        "`max_iter` takes values from 1 to 1000\n",
        "\n",
        "`solver` takes values ['l1', 'l2', 'elasticnet', None]\n",
        "\n",
        "`penalty` takes values ['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga']\n",
        "\n",
        "`max_iter` takes positive values\n",
        "\n",
        "Example:\n",
        "\n",
        "```python\n",
        "\n",
        "log_model2 = LogisticRegression(solver='liblinear',max_iter=500, solver='l2',random_state=41)\n",
        "\n",
        "```\n",
        "\n",
        "Make sure to keep the random_state as 41 or else you may get varying results even with the same code!\n",
        "\n",
        "Create a new variable `log_model2` and change the parameters and see your accuracy!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5d0PaKDLrLe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHndsv-qLrLe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8WGRoTmLrLe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6UwTQZ3LrLf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HwUo_IInLrLf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hOl51dmuLrLf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFFQtK1NLrLn"
      },
      "source": [
        "## SVC\n",
        "\n",
        "Support Vector Machine Classifier\n",
        "\n",
        "There are multiple type of Support Vector Machines:\n",
        "\n",
        "* SVC : Base Model\n",
        "* LinearSVC : Fastest but not most Accurate\n",
        "* SGDClassifier : A Mix of Both\n",
        "\n",
        "The most important ones for Support Vector Machines are\n",
        "\n",
        "* max_iter\n",
        "* multiclass\n",
        "* penalty\n",
        "* loss\n",
        "* C\n",
        "\n",
        "\n",
        "`max_iter` takes values from 1 to 1000\n",
        "\n",
        "`multi_class` takes values ['ovr', 'crammer_singer']\n",
        "\n",
        "`penalty` takes values ['l1', 'l2']\n",
        "\n",
        "`loss` takes values ['hinge', 'squared_hinge']\n",
        "\n",
        "`C` takes positive values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9nzGVWNrLrLn"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import LinearSVC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYJjxQYJLrLn"
      },
      "outputs": [],
      "source": [
        "linsvc_model = LinearSVC()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNoQBqiALrLn"
      },
      "outputs": [],
      "source": [
        "linsvc_model = LinearSVC(max_iter=1000,multi_class='ovr',penalty='l2',loss='squared_hinge',C=1,random_state=41)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5n1dVOwLrLn"
      },
      "outputs": [],
      "source": [
        "linsvc_model.fit(scaled_X_train,y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rrHvxYLTLrLn"
      },
      "outputs": [],
      "source": [
        "linsvc_validation_predictions = linsvc_model.predict(scaled_X_validation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90NbD0bFLrLo"
      },
      "outputs": [],
      "source": [
        "accuracy_score(y_validation,linsvc_validation_predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OPcW09elLrLo"
      },
      "outputs": [],
      "source": [
        "linsvc_holdout_predictions = linsvc_model.predict(scaled_X_holdout_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbIxyztjLrLo"
      },
      "source": [
        "X_validation, X_holdout_test, y_validation, y_holdout_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TAqpSwQ2LrLo"
      },
      "outputs": [],
      "source": [
        "accuracy_score(y_holdout_test,linsvc_holdout_predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gv8yhuI2LrLo"
      },
      "outputs": [],
      "source": [
        "linsvc_y_final_pred = linsvc_model.predict(scaled_X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wcIxInUnLrLo"
      },
      "outputs": [],
      "source": [
        "accuracy_score(y_test,linsvc_y_final_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVHO-UGJLrLo"
      },
      "source": [
        "Parameters you can change!\n",
        "\n",
        "The most important ones for Support Vector Machines are\n",
        "\n",
        "* max_iter\n",
        "* multiclass\n",
        "* penalty\n",
        "* loss\n",
        "* C\n",
        "\n",
        "\n",
        "`max_iter` takes values from 1 to 1000\n",
        "\n",
        "`multi_class` takes values ['ovr', 'crammer_singer']\n",
        "\n",
        "`penalty` takes values ['l1', 'l2']\n",
        "\n",
        "`loss` takes values ['hinge', 'squared_hinge']\n",
        "\n",
        "`C` takes positive values\n",
        "\n",
        "\n",
        "Example:\n",
        "\n",
        "```python\n",
        "\n",
        "svc_model2 = LinearSVC(max_iter=5000,multi_class='crammer singer',penalty='l2',loss='hinge',C=1,random_state=41)\n",
        "\n",
        "```\n",
        "\n",
        "Make sure to keep the random_state as 41 or else you may get varying results even with the same code!\n",
        "\n",
        "Create a new variable `svc_model2` and change the parameters and see your accuracy!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAle4qgwLrLo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bS4O9OBJLrLo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLxfAIzfLrLo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TK7ybY-CLrLo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JQyInjaLrLo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7S-0m2OULrLo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEtwh2VRLrLo"
      },
      "source": [
        "## Random Forest Classifier\n",
        "\n",
        "Random Forest Classifier\n",
        "\n",
        "There are multiple type of Random Forest Classifiers:\n",
        "\n",
        "* Random Forest Classifier : Base Model\n",
        "* ExtraTreesClassifier : Fastest but not most Accurate\n",
        "* GradientBoostingClassifier : Slower but more Accurate\n",
        "\n",
        "The most important ones for Support Vector Machines are\n",
        "\n",
        "* n_estimators\n",
        "* max_depth\n",
        "* criterion\n",
        "\n",
        "\n",
        "`n_estimators` takes values from 1 to 200\n",
        "\n",
        "`max_depth` takes values 1 to 200\n",
        "\n",
        "`criterion` takes values [\"gini\", \"entropy\", \"log_loss\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AkYEiIMSLrLo"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWwpw_Y9LrLo"
      },
      "outputs": [],
      "source": [
        "forest_model = RandomForestClassifier()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ao3q-mIKLrLo"
      },
      "outputs": [],
      "source": [
        "forest_model = RandomForestClassifier(n_estimators = 200, max_depth = 25, criterion ='gini', random_state=41)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ygwNz-jLrLo"
      },
      "outputs": [],
      "source": [
        "forest_model.fit(scaled_X_train,y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2Z-aFxgLrLo"
      },
      "outputs": [],
      "source": [
        "forest_validation_predictions = forest_model.predict(scaled_X_validation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LE9cVo0SLrLp"
      },
      "outputs": [],
      "source": [
        "forest_holdout_predictions = forest_model.predict(scaled_X_holdout_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXmBiReqLrLp"
      },
      "outputs": [],
      "source": [
        "forest_y_final_pred = forest_model.predict(scaled_X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VQBdH6OLrLp"
      },
      "outputs": [],
      "source": [
        "accuracy_score(y_test,forest_y_final_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7KDXxEnULrLp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYEkRbxqLrLp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nugeFuSNLrLp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXRiUwtWLrLp"
      },
      "source": [
        "## Google Form Time!\n",
        "\n",
        "This will take about 5-10 minutes. Make sure to talk with your neighbors about the answers to further your understanding!\n",
        "\n",
        "Google Form: https://forms.gle/UPZQxEbdHJnZyjLc6\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bLB_af0sMrGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qTFL7rGeMr2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y2FpRS54MsYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpVQK17sLrLp"
      },
      "source": [
        "# Neural Network Time\n",
        "\n",
        "Let's make a Neural Network Model that Identifies whether or not the Image is a Number!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s32CAWwDLrLp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Dataset, ConcatDataset, Subset\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EB20nt-4LrLp"
      },
      "source": [
        "Loading the MNIST data into Tensors for our Neural Network to Understand.\n",
        "\n",
        "Tensors are ways to hold data such as:\n",
        "\n",
        "* input data (images, embeddings)\n",
        "* model weights and biases\n",
        "* hidden layers\n",
        "* gradients during backpropagation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KiMmnbROLrLp"
      },
      "outputs": [],
      "source": [
        "# STEP 1: Load the MNIST dataset (images of numbers 0–9)\n",
        "transform = transforms.ToTensor()\n",
        "mnist_full = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__Rh2UVsLrLp"
      },
      "source": [
        "The original dataset consists of 70,000 images and we are reducing back down to 1,000 images to match our previous models. Furthermore, having a smaller subwset with lead to faster experimentation and model training!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "St-Orq0mLrLp"
      },
      "outputs": [],
      "source": [
        "# Select only 1,000 random samples from MNIST\n",
        "subset_indices = np.random.choice(len(mnist_full), 1000, replace=False)\n",
        "mnist = Subset(mnist_full, subset_indices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3dX9EKpLrLp"
      },
      "source": [
        "In this step, we create a custom dataset of random noise images.\n",
        "\n",
        "These will serve as examples of images that are NOT handwritten numbers, allowing us to train our model to distinguish between “numbers” and “not numbers.”\n",
        "\n",
        "Explanation\n",
        "\n",
        "`count:`\n",
        "\n",
        "* The total number of fake images we want to generate.\n",
        "\n",
        "`__len__: `\n",
        "\n",
        "* Returns the number of items in the dataset, which is required for PyTorch datasets.\n",
        "\n",
        "`__getitem__:`\n",
        "\n",
        "* Returns a single random image (img) and its label (label) for a given index idx.\n",
        "\n",
        "`torch.rand(1, 28, 28) `\n",
        "\n",
        "* generates a single-channel (grayscale) image with random pixel values between 0 and 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7X4Gz3CLrLq"
      },
      "source": [
        "We assign a label of 0 because these images are not handwritten digits.\n",
        "\n",
        "* Label 1 = MNIST digit\n",
        "\n",
        "* Label 0 = random noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BI7Oy01KLrLq"
      },
      "outputs": [],
      "source": [
        "# STEP 2: Make fake \"not a number\" images by using random noise\n",
        "class RandomNoise(Dataset):\n",
        "    def __init__(self, count):\n",
        "        self.count = count\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.count\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = torch.rand(1, 28, 28)  # random pixels\n",
        "        label = 0                    # label 0 = not a number\n",
        "        return img, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHXuUW54LrLq"
      },
      "source": [
        "## Activation Layer\n",
        "\n",
        "In this step, we convert the original MNIST dataset into a binary classification format where every image of a handwritten digit is labeled as 1 (“is a number”).\n",
        "\n",
        "`mnist_data:`\n",
        "\n",
        "* The original MNIST dataset (handwritten digits 0–9).\n",
        "\n",
        "`__len__:`\n",
        "\n",
        "* Returns the total number of MNIST images.\n",
        "\n",
        "`__getitem__:`\n",
        "\n",
        "* Retrieves the image at index idx.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3Xsl0ZGLrLq"
      },
      "outputs": [],
      "source": [
        "# STEP 3: Turn MNIST into a dataset of \"is a number\" = 1\n",
        "class MNISTBinary(Dataset):\n",
        "    def __init__(self, mnist_data):\n",
        "        self.mnist_data = mnist_data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.mnist_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, _ = self.mnist_data[idx]\n",
        "        label = 1                    # label 1 = is a number\n",
        "        return img, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e3yWX0OLrLq"
      },
      "source": [
        "In this step, we merge the positive and negative datasets to create a single dataset for training our binary classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dy_pA3ftLrLq"
      },
      "outputs": [],
      "source": [
        "# STEP 4: Combine both datasets together\n",
        "train_data = ConcatDataset([\n",
        "    MNISTBinary(mnist),\n",
        "    RandomNoise(len(mnist))  # same number of fake samples (1000)\n",
        "])\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pa6r8XnPLrLq"
      },
      "source": [
        "## Building our Neural Network!\n",
        "\n",
        "`nn.Flatten()`\n",
        "\n",
        "* Converts each 28×28 image into a 784-dimensional vector.\n",
        "\n",
        "* Input: (1, 28, 28) → Output: (784)\n",
        "\n",
        "`nn.Linear(28*28, 64)`\n",
        "\n",
        "* This is the HIDDEN LAYER: it takes the 784 input features and maps them to 64 neurons.\n",
        "\n",
        "* These 64 neurons are not the output — they are internal representations that the network uses to learn patterns.\n",
        "\n",
        "`nn.ReLU()`\n",
        "\n",
        "* Applies a nonlinear activation to the hidden layer outputs.\n",
        "\n",
        "* ReLU ensures the network can learn nonlinear relationships, not just linear ones.\n",
        "\n",
        "`nn.Linear(64, 1)`\n",
        "\n",
        "* The output layer: reduces 64 hidden features to a single number.\n",
        "\n",
        "`nn.Sigmoid()`\n",
        "\n",
        "* Converts the output to a probability between 0 and 1, suitable for binary classification (number vs. not a number).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1btNbpnSLrLq"
      },
      "outputs": [],
      "source": [
        "# STEP 5: Build a very small neural network\n",
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(28*28, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqNCCxsKLrLq"
      },
      "source": [
        "`model = SimpleNet()`\n",
        "\n",
        "* Creates an instance of our neural network defined in Step 5.\n",
        "\n",
        "* This network takes a 28×28 image as input and outputs a probability that the image is a number (1) or not a number (0).\n",
        "\n",
        "`loss_fn = nn.BCELoss()`\n",
        "\n",
        "* Binary Cross Entropy (BCE) is ideal for binary classification tasks.\n",
        "\n",
        "* Measures how far the predicted probability is from the actual label (0 or 1).\n",
        "\n",
        "* The network will try to minimize this loss during training.\n",
        "\n",
        "`optimizer = optim.Adam(model.parameters(), lr=0.001)`\n",
        "\n",
        "* Adam is a popular optimization algorithm that updates the model’s weights to reduce loss.\n",
        "\n",
        "* `model.parameters()` tells Adam which parameters to update.\n",
        "\n",
        "`lr=0.001`\n",
        "\n",
        "* sets the learning rate, controlling how big each update step is.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pE4RqQrLrLq"
      },
      "outputs": [],
      "source": [
        "# STEP 6: Set up the model, loss, and optimizer\n",
        "model = SimpleNet()\n",
        "loss_fn = nn.BCELoss()               # Binary Cross Entropy = good for yes/no tasks\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oESyfqpGLrLq"
      },
      "outputs": [],
      "source": [
        "# STEP 7: Train the model\n",
        "for epoch in range(3):  # just 3 rounds to keep it fast\n",
        "    total_loss = 0\n",
        "    for imgs, labels in train_loader:\n",
        "        labels = labels.float()\n",
        "        preds = model(imgs).squeeze()\n",
        "        loss = loss_fn(preds, labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}: Loss = {total_loss/len(train_loader):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5zhQUzNLrLq"
      },
      "outputs": [],
      "source": [
        "# STEP 8: Test the model visually\n",
        "model.eval()\n",
        "imgs, labels = next(iter(train_loader))\n",
        "with torch.no_grad():\n",
        "    preds = model(imgs).squeeze()\n",
        "    preds = (preds > 0.5).float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "tjvFcVcmLrLq"
      },
      "outputs": [],
      "source": [
        "# Show first 10 examples\n",
        "fig, axes = plt.subplots(2, 5, figsize=(8,6))\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    ax.imshow(imgs[i].squeeze(), cmap=\"gray\")\n",
        "    ax.set_title(\"Number\" if preds[i]==1 else \"Not a number\")\n",
        "    ax.axis(\"off\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQiNkI_FLrLq"
      },
      "outputs": [],
      "source": [
        "model.eval()  # put model in evaluation mode\n",
        "\n",
        "# Get 1000 samples from the training set (or test set)\n",
        "# Here we just reuse train_loader for simplicity\n",
        "test_images = []\n",
        "test_labels = []\n",
        "\n",
        "for imgs, labels in train_loader:\n",
        "    test_images.append(imgs)\n",
        "    test_labels.append(labels)\n",
        "    if len(torch.cat(test_images)) >= 1000:\n",
        "        break  # stop once we have 1000 images\n",
        "\n",
        "test_images = torch.cat(test_images)[:1000]\n",
        "test_labels = torch.cat(test_labels)[:1000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "HHqC-eJmLrLr"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    preds = model(test_images).squeeze()\n",
        "    preds = (preds > 0.5).float()\n",
        "\n",
        "# STEP 10: Measure accuracy\n",
        "correct = (preds == test_labels).sum().item()\n",
        "accuracy = correct / len(test_labels)\n",
        "print(f\"✅ Accuracy on 1000 images: {accuracy*100:.2f}%\")\n",
        "\n",
        "# STEP 11: See a few random predictions\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Pick 10 random examples\n",
        "idxs = torch.randint(0, 1000, (10,))\n",
        "fig, axes = plt.subplots(2, 5, figsize=(10, 4))\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    idx = idxs[i]\n",
        "    ax.imshow(test_images[idx].squeeze(), cmap=\"gray\")\n",
        "    ax.set_title(\"Number\" if preds[idx]==1 else \"Not a number\")\n",
        "    ax.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCjc87-GLrLr"
      },
      "source": [
        "## Google Form Time Part 2!\n",
        "\n",
        "This will take about 5-10 minutes. Make sure to talk with your neighbors about the answers to further your understanding!\n",
        "\n",
        "Google Form: https://forms.gle/ZZuYoh4xXNEhGaE76\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qe-rGvKZLrLr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkbXgdJELrLr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1X_2b2JLrLr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7fENDudPLrLr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyuM3-RgLrLr"
      },
      "source": [
        "## Making Another Neural Network!\n",
        "\n",
        "Now a Model that not only identifies whether it is a number, but what number it actually is!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17P2Mp-bLrLr"
      },
      "outputs": [],
      "source": [
        "transform = transforms.ToTensor()\n",
        "mnist_full = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
        "subset_indices = np.random.choice(len(mnist_full), 1000, replace=False)\n",
        "mnist_small = Subset(mnist_full, subset_indices)\n",
        "\n",
        "# STEP 2: Create random noise \"not a number\" images\n",
        "class RandomNoise(Dataset):\n",
        "    def __init__(self, count):\n",
        "        self.count = count\n",
        "    def __len__(self):\n",
        "        return self.count\n",
        "    def __getitem__(self, idx):\n",
        "        img = torch.rand(1, 28, 28)\n",
        "        label = 0  # \"not a number\"\n",
        "        return img, label\n",
        "\n",
        "# Convert MNIST digits into \"is a number\" = 1\n",
        "class MNISTBinary(Dataset):\n",
        "    def __init__(self, mnist_data):\n",
        "        self.mnist_data = mnist_data\n",
        "    def __len__(self):\n",
        "        return len(self.mnist_data)\n",
        "    def __getitem__(self, idx):\n",
        "        img, _ = self.mnist_data[idx]\n",
        "        label = 1\n",
        "        return img, label\n",
        "\n",
        "# Combine both for the binary classifier\n",
        "binary_train = ConcatDataset([MNISTBinary(mnist_small), RandomNoise(len(mnist_small))])\n",
        "binary_loader = DataLoader(binary_train, batch_size=64, shuffle=True)\n",
        "\n",
        "# STEP 3: Binary classifier (\"is it a number?\")\n",
        "class BinaryNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(28*28, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "binary_model = BinaryNet()\n",
        "binary_loss = nn.BCELoss()\n",
        "binary_opt = optim.Adam(binary_model.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(3):\n",
        "    total_loss = 0\n",
        "    for imgs, labels in binary_loader:\n",
        "        labels = labels.float()\n",
        "        preds = binary_model(imgs).squeeze()\n",
        "        loss = binary_loss(preds, labels)\n",
        "        binary_opt.zero_grad()\n",
        "        loss.backward()\n",
        "        binary_opt.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}: Binary Loss = {total_loss/len(binary_loader):.4f}\")\n",
        "\n",
        "\n",
        "### =====================================================================\n",
        "#ALL THE NEW CODE!!\n",
        "# STEP 4: Digit classifier (0–9)\n",
        "digit_loader = DataLoader(mnist_small, batch_size=64, shuffle=True)\n",
        "\n",
        "class DigitNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(28*28, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 10)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "digit_model = DigitNet()\n",
        "digit_loss = nn.CrossEntropyLoss()\n",
        "digit_opt = optim.Adam(digit_model.parameters(), lr=0.001)\n",
        "\n",
        "### =====================================================================\n",
        "\n",
        "\n",
        "for epoch in range(3):\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for imgs, labels in digit_loader:\n",
        "        preds = digit_model(imgs)\n",
        "        loss = digit_loss(preds, labels)\n",
        "        digit_opt.zero_grad()\n",
        "        loss.backward()\n",
        "        digit_opt.step()\n",
        "        total_loss += loss.item()\n",
        "        correct += (preds.argmax(1) == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "    print(f\"Epoch {epoch+1}: Digit Loss = {total_loss/len(digit_loader):.4f}, Accuracy = {100*correct/total:.2f}%\")\n",
        "\n",
        "# STEP 5: Test combined system visually\n",
        "binary_model.eval()\n",
        "digit_model.eval()\n",
        "\n",
        "# Make a test batch: half MNIST + half noise\n",
        "test_data = ConcatDataset([Subset(mnist_small, range(5)), RandomNoise(5)])\n",
        "test_loader = DataLoader(test_data, batch_size=1, shuffle=False)\n",
        "\n",
        "fig, axes = plt.subplots(2, 5, figsize=(10, 4))\n",
        "axes = axes.flat\n",
        "\n",
        "print(\"\\nPredictions:\")\n",
        "for i, (img, _) in enumerate(test_loader):\n",
        "    with torch.no_grad():\n",
        "        prob_is_num = binary_model(img).item()\n",
        "        if prob_is_num > 0.5:\n",
        "            # Stage 2: predict which number\n",
        "            pred_digit = digit_model(img).argmax(1).item()\n",
        "            label = f\"Number {pred_digit}\"\n",
        "        else:\n",
        "            label = \"Not a number\"\n",
        "    ax = axes[i]\n",
        "    ax.imshow(img.squeeze(), cmap=\"gray\")\n",
        "    ax.set_title(label)\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQurTXShLrLr"
      },
      "source": [
        "## New Code!\n",
        "\n",
        "The only additional code is\n",
        "\n",
        "```python\n",
        "digit_loader = DataLoader(mnist_small, batch_size=64, shuffle=True)\n",
        "\n",
        "class DigitNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(28*28, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 10)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "digit_model = DigitNet()\n",
        "digit_loss = nn.CrossEntropyLoss()\n",
        "digit_opt = optim.Adam(digit_model.parameters(), lr=0.001)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UvcTpw0LrLr"
      },
      "source": [
        "All that has changed is\n",
        "\n",
        "```python\n",
        "\n",
        "nn.Linear(64, 10)\n",
        "\n",
        "...\n",
        "\n",
        "digit_loss = nn.CrossEntropyLoss()\n",
        "```\n",
        "\n",
        "`nn.Linear(64, 10)` this line of code takes our output of a 64 1D Vector and determines what number from 0-9 (10 number) it is\n",
        "\n",
        "`digit_loss = nn.CrossEntropyLoss()` our previous Loss function as Binary Cross Entropy or (BCE) and now it is Ctegorical Cross Entropy Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAaZswBGLrLr"
      },
      "source": [
        "### What happens if we do 5 or more epochs?\n",
        "\n",
        "```python\n",
        "for epoch in range(...):\n",
        "```\n",
        "\n",
        "If you want to see more than 5 epochs, change the number in the range function to your desired number!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URyIvmObLrLr"
      },
      "source": [
        "## Model for 5 Epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "PzIpPVAKLrLr"
      },
      "outputs": [],
      "source": [
        "transform = transforms.ToTensor()\n",
        "mnist_full = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
        "subset_indices = np.random.choice(len(mnist_full), 1000, replace=False)\n",
        "mnist_small = Subset(mnist_full, subset_indices)\n",
        "\n",
        "# STEP 2: Create random noise \"not a number\" images\n",
        "class RandomNoise(Dataset):\n",
        "    def __init__(self, count):\n",
        "        self.count = count\n",
        "    def __len__(self):\n",
        "        return self.count\n",
        "    def __getitem__(self, idx):\n",
        "        img = torch.rand(1, 28, 28)\n",
        "        label = 0  # \"not a number\"\n",
        "        return img, label\n",
        "\n",
        "# Convert MNIST digits into \"is a number\" = 1\n",
        "class MNISTBinary(Dataset):\n",
        "    def __init__(self, mnist_data):\n",
        "        self.mnist_data = mnist_data\n",
        "    def __len__(self):\n",
        "        return len(self.mnist_data)\n",
        "    def __getitem__(self, idx):\n",
        "        img, _ = self.mnist_data[idx]\n",
        "        label = 1\n",
        "        return img, label\n",
        "\n",
        "# Combine both for the binary classifier\n",
        "binary_train = ConcatDataset([MNISTBinary(mnist_small), RandomNoise(len(mnist_small))])\n",
        "binary_loader = DataLoader(binary_train, batch_size=64, shuffle=True)\n",
        "\n",
        "# STEP 3: Binary classifier (\"is it a number?\")\n",
        "class BinaryNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(28*28, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "binary_model = BinaryNet()\n",
        "binary_loss = nn.BCELoss()\n",
        "binary_opt = optim.Adam(binary_model.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(5):\n",
        "    total_loss = 0\n",
        "    for imgs, labels in binary_loader:\n",
        "        labels = labels.float()\n",
        "        preds = binary_model(imgs).squeeze()\n",
        "        loss = binary_loss(preds, labels)\n",
        "        binary_opt.zero_grad()\n",
        "        loss.backward()\n",
        "        binary_opt.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}: Binary Loss = {total_loss/len(binary_loader):.4f}\")\n",
        "\n",
        "\n",
        "### =====================================================================\n",
        "#ALL THE NEW CODE!!\n",
        "# STEP 4: Digit classifier (0–9)\n",
        "digit_loader = DataLoader(mnist_small, batch_size=64, shuffle=True)\n",
        "\n",
        "class DigitNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(28*28, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 10)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "digit_model = DigitNet()\n",
        "digit_loss = nn.CrossEntropyLoss()\n",
        "digit_opt = optim.Adam(digit_model.parameters(), lr=0.001)\n",
        "\n",
        "### =====================================================================\n",
        "\n",
        "\n",
        "for epoch in range(5):\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for imgs, labels in digit_loader:\n",
        "        preds = digit_model(imgs)\n",
        "        loss = digit_loss(preds, labels)\n",
        "        digit_opt.zero_grad()\n",
        "        loss.backward()\n",
        "        digit_opt.step()\n",
        "        total_loss += loss.item()\n",
        "        correct += (preds.argmax(1) == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "    print(f\"Epoch {epoch+1}: Digit Loss = {total_loss/len(digit_loader):.4f}, Accuracy = {100*correct/total:.2f}%\")\n",
        "\n",
        "# STEP 5: Test combined system visually\n",
        "binary_model.eval()\n",
        "digit_model.eval()\n",
        "\n",
        "# Make a test batch: half MNIST + half noise\n",
        "test_data = ConcatDataset([Subset(mnist_small, range(5)), RandomNoise(5)])\n",
        "test_loader = DataLoader(test_data, batch_size=1, shuffle=False)\n",
        "\n",
        "fig, axes = plt.subplots(2, 5, figsize=(10, 4))\n",
        "axes = axes.flat\n",
        "\n",
        "print(\"\\nPredictions:\")\n",
        "for i, (img, _) in enumerate(test_loader):\n",
        "    with torch.no_grad():\n",
        "        prob_is_num = binary_model(img).item()\n",
        "        if prob_is_num > 0.5:\n",
        "            # Stage 2: predict which number\n",
        "            pred_digit = digit_model(img).argmax(1).item()\n",
        "            label = f\"Number {pred_digit}\"\n",
        "        else:\n",
        "            label = \"Not a number\"\n",
        "    ax = axes[i]\n",
        "    ax.imshow(img.squeeze(), cmap=\"gray\")\n",
        "    ax.set_title(label)\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZoRr6UuLrLr"
      },
      "source": [
        "## Test Model for X Epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDBIcqHCLrLr"
      },
      "outputs": [],
      "source": [
        "transform = transforms.ToTensor()\n",
        "mnist_full = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
        "subset_indices = np.random.choice(len(mnist_full), 1000, replace=False)\n",
        "mnist_small = Subset(mnist_full, subset_indices)\n",
        "\n",
        "# STEP 2: Create random noise \"not a number\" images\n",
        "class RandomNoise(Dataset):\n",
        "    def __init__(self, count):\n",
        "        self.count = count\n",
        "    def __len__(self):\n",
        "        return self.count\n",
        "    def __getitem__(self, idx):\n",
        "        img = torch.rand(1, 28, 28)\n",
        "        label = 0  # \"not a number\"\n",
        "        return img, label\n",
        "\n",
        "# Convert MNIST digits into \"is a number\" = 1\n",
        "class MNISTBinary(Dataset):\n",
        "    def __init__(self, mnist_data):\n",
        "        self.mnist_data = mnist_data\n",
        "    def __len__(self):\n",
        "        return len(self.mnist_data)\n",
        "    def __getitem__(self, idx):\n",
        "        img, _ = self.mnist_data[idx]\n",
        "        label = 1\n",
        "        return img, label\n",
        "\n",
        "# Combine both for the binary classifier\n",
        "binary_train = ConcatDataset([MNISTBinary(mnist_small), RandomNoise(len(mnist_small))])\n",
        "binary_loader = DataLoader(binary_train, batch_size=64, shuffle=True)\n",
        "\n",
        "# STEP 3: Binary classifier (\"is it a number?\")\n",
        "class BinaryNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(28*28, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "binary_model = BinaryNet()\n",
        "binary_loss = nn.BCELoss()\n",
        "binary_opt = optim.Adam(binary_model.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(...):\n",
        "    total_loss = 0\n",
        "    for imgs, labels in binary_loader:\n",
        "        labels = labels.float()\n",
        "        preds = binary_model(imgs).squeeze()\n",
        "        loss = binary_loss(preds, labels)\n",
        "        binary_opt.zero_grad()\n",
        "        loss.backward()\n",
        "        binary_opt.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}: Binary Loss = {total_loss/len(binary_loader):.4f}\")\n",
        "\n",
        "\n",
        "### =====================================================================\n",
        "#ALL THE NEW CODE!!\n",
        "# STEP 4: Digit classifier (0–9)\n",
        "digit_loader = DataLoader(mnist_small, batch_size=64, shuffle=True)\n",
        "\n",
        "class DigitNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(28*28, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 10)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "digit_model = DigitNet()\n",
        "digit_loss = nn.CrossEntropyLoss()\n",
        "digit_opt = optim.Adam(digit_model.parameters(), lr=0.001)\n",
        "\n",
        "### =====================================================================\n",
        "\n",
        "\n",
        "for epoch in range(...):\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for imgs, labels in digit_loader:\n",
        "        preds = digit_model(imgs)\n",
        "        loss = digit_loss(preds, labels)\n",
        "        digit_opt.zero_grad()\n",
        "        loss.backward()\n",
        "        digit_opt.step()\n",
        "        total_loss += loss.item()\n",
        "        correct += (preds.argmax(1) == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "    print(f\"Epoch {epoch+1}: Digit Loss = {total_loss/len(digit_loader):.4f}, Accuracy = {100*correct/total:.2f}%\")\n",
        "\n",
        "# STEP 5: Test combined system visually\n",
        "binary_model.eval()\n",
        "digit_model.eval()\n",
        "\n",
        "# Make a test batch: half MNIST + half noise\n",
        "test_data = ConcatDataset([Subset(mnist_small, range(5)), RandomNoise(5)])\n",
        "test_loader = DataLoader(test_data, batch_size=1, shuffle=False)\n",
        "\n",
        "fig, axes = plt.subplots(2, 5, figsize=(10, 4))\n",
        "axes = axes.flat\n",
        "\n",
        "print(\"\\nPredictions:\")\n",
        "for i, (img, _) in enumerate(test_loader):\n",
        "    with torch.no_grad():\n",
        "        prob_is_num = binary_model(img).item()\n",
        "        if prob_is_num > 0.5:\n",
        "            # Stage 2: predict which number\n",
        "            pred_digit = digit_model(img).argmax(1).item()\n",
        "            label = f\"Number {pred_digit}\"\n",
        "        else:\n",
        "            label = \"Not a number\"\n",
        "    ax = axes[i]\n",
        "    ax.imshow(img.squeeze(), cmap=\"gray\")\n",
        "    ax.set_title(label)\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BvmnCoYmJSzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AI School 2 Post-Test Survey\n",
        "\n",
        "Form: https://forms.gle/jLmmB61b4dxZpErz5\n"
      ],
      "metadata": {
        "id": "UTifPX1PJVXN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wnXwHv5ELrLr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0n-sHmAxLrLs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6V6bagcLrLs"
      },
      "source": [
        "# Post-AI School Stuff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8aW25GrLrLs"
      },
      "source": [
        "## Logistic Regression Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "Lov3QelDLrLs"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "log_model = LogisticRegression(random_state=41)\n",
        "penalty = ['none','l1','l2','elasticnet']\n",
        "solver = ['lbfgs','liblinear','saga']\n",
        "max_iter = [500, 1000]\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "param_grid = {'penalty': penalty,\n",
        "              'solver' : solver,\n",
        "              'max_iter': max_iter}\n",
        "lr_grid_model = GridSearchCV(log_model,param_grid=param_grid)\n",
        "lr_grid_model.fit(scaled_X_train,y_train)\n",
        "lr_grid_model.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c30XDdZjLrLs"
      },
      "outputs": [],
      "source": [
        "best_lr_model = LogisticRegression(C=..., max_iter=..., penalty=...,random_state=41)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "best_lr_model.fit(scaled_X_train,y_train)\n",
        "scaler = StandardScaler()\n",
        "scaled_X_train = scaler.fit_transform(X_train)\n",
        "scaled_X_test = scaler.transform(X_test)\n",
        "scaled_X_holdout_test = scaler.transform(X_holdout_test)\n",
        "scaled_X_validation = scaler.fit_transform(X_validation)\n",
        "lr_validation_predictions = best_lr_model.predict(scaled_X_validation)\n",
        "lr_holdout_predictions = best_lr_model.predict(scaled_X_holdout_test)\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "accuracy_score(y_holdout_test,lr_holdout_predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Yn0VsiHLrLs"
      },
      "source": [
        "## LinearSVC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "9Hu0xlz6LrLs"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "penality = ['l1', 'l2']\n",
        "C = [0.01, 0.1, 1, 10]\n",
        "max_iter = [100, 500, 1000]\n",
        "param_grid = {'penalty': penalty,\n",
        "              'C': C,\n",
        "             'max_iter': max_iter}\n",
        "linsvc_model = LinearSVC(random_state=41)\n",
        "svc_grid_model = GridSearchCV(linsvc_model,param_grid=param_grid)\n",
        "svc_grid_model.fit(scaled_X_train, y_train)\n",
        "best_svc_model = svc_grid_model.best_params_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AfkuZVLZLrLs"
      },
      "outputs": [],
      "source": [
        "best_svc_model = LinearSVC(random_state=41,C=...,max_iter=..., penalty=...)\n",
        "best_svc_model.fit(scaled_X_train,y_train)\n",
        "svc_validation_predictions = best_svc_model.predict(scaled_X_validation)\n",
        "svc_holdout_predictions = best_svc_model.predict(scaled_X_holdout_test)\n",
        "svc_y_final_pred = best_svc_model.predict(scaled_X_test)\n",
        "accuracy_score(y_test,svc_y_final_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suuwKaYaLrLs"
      },
      "source": [
        "## Random Forest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fE8B37jfLrLs"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "forest_model = RandomForestClassifier(random_state=41)\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['sqrt', 'log2']\n",
        "}\n",
        "forest_grid_model = GridSearchCV(estimator=forest_model,param_grid=param_grid)\n",
        "forest_grid_model.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngxWD-29LrLs"
      },
      "outputs": [],
      "source": [
        "best_forest_model = RandomForestClassifier(n_estimators = ..., max_depth = ..., max_features =...,random_state=41)\n",
        "best_forest_model_.fit(scaled_X_train, y_train)\n",
        "best_forest\n",
        "forest_validation_predictions = best_forest_model.predict(scaled_X_validation)\n",
        "forest_holdout_predictions = best_forest_model.predict(scaled_X_holdout_test)\n",
        "forest_y_final_pred = forest_model.predict(scaled_X_test)\n",
        "accuracy_score(y_test,forest_y_final_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIQ3gWsuLrLs"
      },
      "source": [
        "## Neuron Network Identifying Which Number"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ih7F88PjLrLs"
      },
      "outputs": [],
      "source": [
        "transform = transforms.ToTensor()\n",
        "mnist_full = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
        "subset_indices = np.random.choice(len(mnist_full), 1000, replace=False)\n",
        "mnist_small = Subset(mnist_full, subset_indices)\n",
        "\n",
        "class RandomNoise(Dataset):\n",
        "    def __init__(self, count):\n",
        "        self.count = count\n",
        "    def __len__(self):\n",
        "        return self.count\n",
        "    def __getitem__(self, idx):\n",
        "        img = torch.rand(1, 28, 28)\n",
        "        label = 0  # \"not a number\"\n",
        "        return img, label\n",
        "\n",
        "class MNISTBinary(Dataset):\n",
        "    def __init__(self, mnist_data):\n",
        "        self.mnist_data = mnist_data\n",
        "    def __len__(self):\n",
        "        return len(self.mnist_data)\n",
        "    def __getitem__(self, idx):\n",
        "        img, _ = self.mnist_data[idx]\n",
        "        label = 1\n",
        "        return img, label\n",
        "\n",
        "binary_train = ConcatDataset([MNISTBinary(mnist_small), RandomNoise(len(mnist_small))])\n",
        "binary_loader = DataLoader(binary_train, batch_size=64, shuffle=True)\n",
        "\n",
        "class BinaryNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(28*28, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "binary_model = BinaryNet()\n",
        "binary_loss = nn.BCELoss()\n",
        "binary_opt = optim.Adam(binary_model.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(5):\n",
        "    total_loss = 0\n",
        "    for imgs, labels in binary_loader:\n",
        "        labels = labels.float()\n",
        "        preds = binary_model(imgs).squeeze()\n",
        "        loss = binary_loss(preds, labels)\n",
        "        binary_opt.zero_grad()\n",
        "        loss.backward()\n",
        "        binary_opt.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}: Binary Loss = {total_loss/len(binary_loader):.4f}\")\n",
        "\n",
        "\n",
        "digit_loader = DataLoader(mnist_small, batch_size=64, shuffle=True)\n",
        "\n",
        "class DigitNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(28*28, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 10)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "digit_model = DigitNet()\n",
        "digit_loss = nn.CrossEntropyLoss()\n",
        "digit_opt = optim.Adam(digit_model.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(5):\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for imgs, labels in digit_loader:\n",
        "        preds = digit_model(imgs)\n",
        "        loss = digit_loss(preds, labels)\n",
        "        digit_opt.zero_grad()\n",
        "        loss.backward()\n",
        "        digit_opt.step()\n",
        "        total_loss += loss.item()\n",
        "        correct += (preds.argmax(1) == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "    print(f\"Epoch {epoch+1}: Digit Loss = {total_loss/len(digit_loader):.4f}, Accuracy = {100*correct/total:.2f}%\")\n",
        "\n",
        "binary_model.eval()\n",
        "digit_model.eval()\n",
        "\n",
        "test_data = ConcatDataset([Subset(mnist_small, range(5)), RandomNoise(5)])\n",
        "test_loader = DataLoader(test_data, batch_size=1, shuffle=False)\n",
        "\n",
        "fig, axes = plt.subplots(2, 5, figsize=(10, 4))\n",
        "axes = axes.flat\n",
        "\n",
        "print(\"\\nPredictions:\")\n",
        "for i, (img, _) in enumerate(test_loader):\n",
        "    with torch.no_grad():\n",
        "        prob_is_num = binary_model(img).item()\n",
        "        if prob_is_num > 0.5:\n",
        "            # Stage 2: predict which number\n",
        "            pred_digit = digit_model(img).argmax(1).item()\n",
        "            label = f\"Number {pred_digit}\"\n",
        "        else:\n",
        "            label = \"Not a number\"\n",
        "    ax = axes[i]\n",
        "    ax.imshow(img.squeeze(), cmap=\"gray\")\n",
        "    ax.set_title(label)\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}